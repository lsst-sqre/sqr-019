{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "# Verification Framework API Demonstration\n",
    "\n",
    "**[SQR-019](https://sqr-019.lsst.io)**\n",
    "\n",
    "Jonathan Sick, LSST/AURA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "LSST Data Management's verification program ensures that code meets performance specifications during construction. During operations, verification will continue, with an additional focus towards ensuring that data releases meet requirements for science. To facilitate verification activities, we are introducing the LSST Verification Framework, implemented in the `lsst.verify` Python package. This technical note introduces the framework's concepts and usage patterns through a working tutorial. We define metrics (observable concepts), specification (requirements and milestones that metric measurements should meet). Then we measure metrics, using both a lightweight approach that is easy to retrofit into LSST Science Pipelines Tasks and a more rigorous measurement approach that enables detailed diagnostics. Finally, this tutorial shows how metric measurements can be analyzed in a Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "This technical note is available as a Jupyter notebook from its GitHub repository: https://github.com/lsst-sqre/sqr-019. You are encouraged to run and modify this notebook to help you learn about the `lsst.verify` package. This section will help you download dependencies for running this notebook.\n",
    "\n",
    "First, [install the LSST Science Pipelines with lsstsw](https://pipelines.lsst.io/install/lsstsw.html). Specifically, build and `setup` the `verify` package:\n",
    "\n",
    "```bash\n",
    "rebuild verify\n",
    "setup verify\n",
    "```\n",
    "\n",
    "*The `verify` package is not yet distributed with the LSST Science Pipelines Stack as of this writing. lsstsw is the most convient means of installing `verify` from scratch.*\n",
    "\n",
    "Then install these packages with Anaconda:\n",
    "\n",
    "```bash\n",
    "conda install jupyter pandas bokeh\n",
    "```\n",
    "\n",
    "These additional packages are used for this technote note, but are required to use `lsst.verify` itself.\n",
    "\n",
    "These are the Python imports needed for this technical note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"f2049a89-1e37-413b-8008-2c5abcac7793\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"f2049a89-1e37-413b-8008-2c5abcac7793\");\n",
       "      el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"f2049a89-1e37-413b-8008-2c5abcac7793\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f2049a89-1e37-413b-8008-2c5abcac7793' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"f2049a89-1e37-413b-8008-2c5abcac7793\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.5.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.5.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"f2049a89-1e37-413b-8008-2c5abcac7793\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard library and third party packages used by this notebook\n",
    "import json\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# For demonstration  plots\n",
    "from bokeh.charts import Scatter\n",
    "from bokeh.models import Range1d, Span\n",
    "from bokeh.layouts import row\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "import pandas\n",
    "\n",
    "# Load Bokeh\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Verification Framework itself\n",
    "import lsst.verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[lsst.verify](https://github.com/lsst/verify) is the new framework for making verification measurements in the LSST Science Pipelines. *Verification* is an activity where well-defined quantities, called *metrics*, are measured to ensure that LSST's data and pipelines meet requirements, which we call *specifications*.\n",
    "\n",
    "You might be familar with [validate_drp](https://github.com/lsst/validate_drp). That package currently measures metrics of `ProcessCcdTask` outputs and posts results to [SQUASH](https://squash.lsst.codes). By tracking metric measurements we are able to understand trends in the algorithmic performance of the LSST Science Pipelines, and ultimately verify that we will meet our requirements.\n",
    "\n",
    "With `lsst.verify` we sought to generalize the process of defining metrics, measuring those metrics, and tracking those measurements in SQUASH. Rather than supporting only specially-design verification afterburner tasks, our goal is to empower developers to track performance metrics of their own specific pipeline Tasks. By defining metrics relevant to specific Tasks, verification becomes a highly relevant integration testing activity for day-to-day pipelines development.\n",
    "\n",
    "This tutorial demonstrates key features and patterns in the `lsst.verify` framework, from defining metrics and specifications, to making measurements, to analyzing and summarizing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metrics\n",
    "\n",
    "Metrics are definitions of measurable things that you want to track. A measureable thing could be anything: the $\\chi^2$ of a fit, the number of sources identified and measured, or even the latency or memory usage of a function.\n",
    "\n",
    "In the verification framework, all metrics are centrally defined in the [verify_metrics](https://github.com/lsst/verify_metrics) package. To define a new metric, simply add or modify a YAML file in the `/metrics` directory of [verify_metrics](https://github.com/lsst/verify_metrics). Each Stack package that measures metrics has its own YAML definitions file (`jointcal.yaml`, `validate_drp.yaml`, and so on).\n",
    "\n",
    "SQUASH watches `verify_metrics` so that when a metric is committed to the GitHub repo is it also known to the SQUASH dashboard.\n",
    "\n",
    "For this tutorial, we will create metrics for hypothetical `demo1` and `demo2` packages. Here are some metric definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, content for a hypothetical `/metrics/demo1.yaml` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo1_metrics_yaml = \"\"\"\n",
    "ZeropointRMS:\n",
    "  unit: mmag\n",
    "  description: >\n",
    "    Photometric calibration RMS.\n",
    "  reference:\n",
    "    url: https://example.com/PhotRMS\n",
    "  tags:\n",
    "    - photometry\n",
    "    - demo\n",
    "\n",
    "Completeness:\n",
    "  unit: mag\n",
    "  description: >\n",
    "    Magnitude of the catalog's 50% completeness limit.\n",
    "  reference:\n",
    "    url: https://example.com/Complete\n",
    "  tags:\n",
    "    - photometry\n",
    "    - demo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This YAML defines three metrics: `demo1.ZeropointRMS`, `demo1.Complete` and `demo1.SourceCount`. A metric consists of:\n",
    "\n",
    "- **A name.** Names are prefixed by name of the package that defines them.\n",
    "- **A description.** This helps to document metrics, even if they are more thoroughly defined in other documentation (see the `reference` field).\n",
    "- **A unit.** Units are `astropy.units`-compatibble strings. Metrics of unitless quantities should use the [dimensionless_unscaled](http://docs.astropy.org/en/stable/units/standard_units.html#the-dimensionless-unit) unit, an empty string.\n",
    "- **References.** References can be made to URLs, or even to document handles and page numbers. We can expand the `reference` field's schema to accomodate formalize reference identifiers in the future.\n",
    "- **Tags.** These help us group metrics together in reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this demo, we'll parse this YAML object into a `lsst.verify.MetricSet` collection. Normally this doesn't need to be done since metrics should be pre-defined in `verify_metrics`, which are automatically loaded as we'll see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=2&gt;\n",
       "<table id=\"table4685324184\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Name</th><th>Description</th><th>Units</th><th>Reference</th><th>Tags</th></tr></thead>\n",
       "<thead><tr><th>str18</th><th>str50</th><th>str15</th><th>str28</th><th>str16</th></tr></thead>\n",
       "<tr><td>demo1.Completeness</td><td>Magnitude of the catalog&apos;s 50% completeness limit.</td><td>$\\mathrm{mag}$</td><td>https://example.com/Complete</td><td>demo, photometry</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS</td><td>Photometric calibration RMS.</td><td>$\\mathrm{mmag}$</td><td>https://example.com/PhotRMS</td><td>demo, photometry</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<lsst.verify.metricset.MetricSet at 0x117445da0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    demo1_metrics_path = os.path.join(temp_dir, 'demo1.yaml')\n",
    "    with open(demo1_metrics_path, mode='w') as f:\n",
    "        f.write(demo1_metrics_yaml)\n",
    "    demo_metrics = lsst.verify.MetricSet.load_single_package(demo1_metrics_path)\n",
    "demo_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metric specifications\n",
    "\n",
    "Specifications are tests of metric measurements. A specification can be thought of as a milestone; if a measurement passes a specification then data and code are working as expected.\n",
    "\n",
    "Like metrics, specifications can be centrally defined in the [verify_metrics](https://github.com/lsst/verify_metrics) repository. Specifications for each package are defined in one or more YAML files in the `specs` subdirectory of `verify_metrics`. See the [validate_drp directory for an example](https://github.com/lsst/verify_metrics/tree/master/specs/validate_drp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a typical specification, in this case for the `demo1.ZeropointRMS` metric:\n",
    "\n",
    "```yaml\n",
    "name: \"minimum\"\n",
    "metric: \"ZeropointRMS\"\n",
    "threshold:\n",
    "  operator: \"<=\"\n",
    "  unit: \"mmag\"\n",
    "  value: 20.0\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "```\n",
    "\n",
    "The fully-qualified name for this specification is `demo1.ZeropointRMS.minimum`, following a `{package}.{metric}.{spec_name}` format. Specifications names should be unique, but otherwise can be anything. The Verification Framework does not place special meaning on \"minimum,\" \"design,\" and \"stretch\" specifications. Instead, we recommend that you use tags to designate specifiations with operational meaning.\n",
    "\n",
    "The core of a specification is its test, and the `demo1.ZeropointRMS.minimum` specification defines its test in the `threshold` YAML field. Here, a measurement *passes* the specification if $\\mathrm{measurement} \\leq 20.0~\\mathrm{mmag}$.\n",
    "\n",
    "We envision other types of specifications beyond thresholds (binary comparisions). Possible specification types include ranges and tolerances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata queries: making specifications only act upon certain measurements\n",
    "\n",
    "Often you'll make measurements of a metric in many contexts. With different datasets, from different cameras, in different filters. A specification we define for one measurement context might not be relevant for other contexts. LPM-17, for example, does this frequently by defining different specifications for $gri$ datasets than $uzy$. To prevent false alerts, the Verification Framework allows you to define criteria for when a specification applies to a measurement.\n",
    "\n",
    "Originally we indended to leverage the *provenance* of a pipeline execution. Provenance, in general, fully describes the environment of the pipeline run, the datasets that were processed and produced, and the pipeline configuration. We envisioned that specifications might query the provenance of a metric measurement to determine if the specification's test is applicable. While this is our long term design intent, a comprehensive pipeline provenance framework does not exist.\n",
    "\n",
    "To shim the provenance system's functionality, the Verification Framework introduces a complementary concept called *job metadata*. Whereas provenance is passively gathered during pipeline execution, metadata is explicitly added by pipeline developers and operators. Metadata could be a task configuration, filter name, dataset name, or any state known during a Task's execution.\n",
    "\n",
    "For example, suppose that a specification only applies to CFHT/MegaCam datasets in the $r$-band. This requirement is written into the specification's definition with a `metadata_query` field:\n",
    "\n",
    "```yaml\n",
    "name: \"minimum_megacam_r\"\n",
    "metric: \"ZeropointRMS\"\n",
    "threshold:\n",
    "  operator: \"<=\"\n",
    "  unit: \"mmag\"\n",
    "  value: 20.0\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "metadata_query:\n",
    "  camera: \"megacam\"\n",
    "  filter_name: \"r\" \n",
    "```\n",
    "\n",
    "If a job has metadata with *matching* `camera` and `filter_name` fields, the specification applies:\n",
    "\n",
    "```json\n",
    "{\n",
    "  'camera': 'megacam',\n",
    "  'filter_name': 'r'\n",
    "  'dataset_repo': 'https://github.com/lsst/ci_cfht.git'\n",
    "}\n",
    "```\n",
    "\n",
    "On the other hand, if a job has metadata that is either missing fields, or has conflicting values, the specification does not apply:\n",
    "\n",
    "```json\n",
    "{\n",
    "  'filter_name': 'i'\n",
    "  'dataset_repo': 'https://github.com/lsst/ci_cfht.git'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Specification inheritance\n",
    "\n",
    "Metadata queries help us write specifications that monitor precisely the pipeline runs we are interested in, with test criteria that make sense. But this also means that we are potentially writing many more specifications for each metric. Most specifications for a given metric share common characteristics, such as units, threshold operators, the metric name, and even some base metadata query terms. To write specifications without repeating outself, we can take advantage of specification inheritance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's write a basic specification in YAML for the `demo1.ZeropointRMS` metric, and write another specification that is customized for CFHT/MegaCam $r$-band data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=2&gt;\n",
       "<table id=\"table4655628128\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Name</th><th>Test</th><th>Tags</th></tr></thead>\n",
       "<thead><tr><th>str36</th><th>str27</th><th>str7</th></tr></thead>\n",
       "<tr><td>demo1.ZeropointRMS.minimum</td><td>$x$ &lt;= 20.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<lsst.verify.specset.SpecificationSet at 0x117445b38>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeropointrms_specs_yaml = \"\"\"\n",
    "---\n",
    "name: \"minimum\"\n",
    "metric: \"ZeropointRMS\"\n",
    "threshold:\n",
    "  operator: \"<=\"\n",
    "  unit: \"mmag\"\n",
    "  value: 20.0\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "\n",
    "---\n",
    "name: \"minimum_megacam_r\"\n",
    "base: [\"ZeropointRMS.minimum\"]\n",
    "threshold:\n",
    "  value: 15.0\n",
    "metadata_query:\n",
    "  camera: \"megacam\"\n",
    "  filter_name: \"r\"\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    # Write YAML to disk, emulating the verify_metrics package for this demo\n",
    "    specs_dirname = os.path.join(temp_dir, 'demo1')\n",
    "    os.makedirs(specs_dirname)\n",
    "    demo1_specs_path = os.path.join(specs_dirname, 'zeropointRMS.yaml')\n",
    "    with open(demo1_specs_path, mode='w') as f:\n",
    "        f.write(zeropointrms_specs_yaml)\n",
    "\n",
    "    # Parse the YAML into a set of Specification objects\n",
    "    demo1_specs = lsst.verify.SpecificationSet.load_single_package(specs_dirname)\n",
    "\n",
    "demo1_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `demo1.ZeropointRMS.minimum_megacam_r` specification indicates that it inherits from `demo1.ZeropointRMS.minium` by referencing it in the `base` field.\n",
    "\n",
    "With inheritance, `demo1.ZeropointRMS.minimum_megacam_r` includes all fields defined in its base, adds new fields, and overrides values. Notice how the threshold has changed from 20.0 mmag, to 15.0 mmag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification partials for even more composable specifications\n",
    "\n",
    "Suppose we want to create specifications for many metrics that apply to the `megacam` camera. Specification inheritance doesn't help because we need to repeat the metadata query for each metric:\n",
    "\n",
    "```yaml\n",
    "---\n",
    "# Base specification: demo1.ZeropointRMS.minimum\n",
    "name: \"minimum\"\n",
    "metric: \"ZeropointRMS\"\n",
    "threshold:\n",
    "  operator: \"<=\"\n",
    "  unit: \"mmag\"\n",
    "  value: 20.0\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "\n",
    "---\n",
    "# Base specification: demo1.Completeness.minimum\n",
    "name: \"minimum\"\n",
    "metric: \"Completeness\"\n",
    "threshold:\n",
    "  operator: \">=\"\n",
    "  unit: \"mag\"\n",
    "  value: 20.0\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "  \n",
    "---\n",
    "# A demo1.ZeropointRMS specification targetting MegaCam r-band\n",
    "name: \"minimum_megacam_r\"\n",
    "base: [\"ZeropointRMS.minimum\"]\n",
    "threshold:\n",
    "  value: 15.0\n",
    "metadata_query:\n",
    "  camera: \"megacam\"\n",
    "  filter_name: \"r\"\n",
    "\n",
    "---\n",
    "# A demo1.CompletenessRMS specification targetting MegaCam r-band\n",
    "name: \"minimum_megacam_r\"\n",
    "base: [\"Completeness.minimum\"]\n",
    "threshold:\n",
    "  value: 24.0\n",
    "metadata_query:\n",
    "  camera: \"megacam\"\n",
    "  filter_name: \"r\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid duplicating `metadata_query` information for all MegaCam $r$-band specifications across many metrics, we can extract that information into a **partial.** Partials are formatted like specifications, but are never parsed as stand-alone specifiations. That means a partial can, as the name implies, define common partial information that can be mixed into many specifications.\n",
    "\n",
    "Here's the same example as before, but written with a `#megacam_r` partial:\n",
    "\n",
    "```yaml\n",
    "---\n",
    "# Partial for MegaCam r-band specifications\n",
    "id: \"megacam-r\"\n",
    "metadata_query:\n",
    "  camera: \"megacam\"\n",
    "  filter_name: \"r\"\n",
    "\n",
    "---\n",
    "# Base specification: demo1.ZeropointRMS.minimum\n",
    "name: \"minimum\"\n",
    "metric: \"ZeropointRMS\"\n",
    "threshold:\n",
    "  operator: \"<=\"\n",
    "  unit: \"mmag\"\n",
    "  value: 20.0\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "\n",
    "---\n",
    "# Base specification: demo1.Completeness.minimum\n",
    "name: \"minimum\"\n",
    "metric: \"Completeness\"\n",
    "threshold:\n",
    "  operator: \">=\"\n",
    "  unit: \"mag\"\n",
    "  value: 20.0\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "  \n",
    "---\n",
    "# A demo1.ZeropointRMS specification targetting MegaCam r-band\n",
    "name: \"minimum_megacam_r\"\n",
    "base: [\"ZeropointRMS.minimum\", \"#megacam-r\"]\n",
    "threshold:\n",
    "  value: 15.0\n",
    "\n",
    "\n",
    "---\n",
    "# A demo1.Completeness specification targetting MegaCam r-band\n",
    "name: \"minimum_megacam_r\"\n",
    "base: [\"Completeness.minimum\", \"#megacam-r]\n",
    "threshold:\n",
    "  value: 24.0\n",
    "```\n",
    "\n",
    "As you can see, we've added the `megacam-r` partial to the inheritance chain defined in the `base` fields. The `demo1.ZeropointRMS.minimum_megacam_r` and `demo1.Completeness.minimum_megacam_r` specifications inherit form both specifications *and* the `#megacam-r` partial. The `#` prefix implies an partial, not a specification. It's also possible to reference partials in other YAML files, see the [validate_drp specifications](https://github.com/lsst/verify_metrics/blob/master/specs/validate_drp/cfht_gri/PA1.yaml) for an example.\n",
    "\n",
    "Inheritance is evaluated left to right. For example, `demo1.Completeness.minimum_megacam_r` is built up in this order:\n",
    "\n",
    "1. Use the `demo1.Completeness.minimum` specification.\n",
    "2. Override with information from `#megacam-r`.\n",
    "3. Override with information from the `demo1.Completeness.minimum` specification's own YAML fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifications: putting it all together\n",
    "\n",
    "We've seen how to write specification metrics in YAML, and how to write them more efficiently with inheritance and partials. Now let's write out a full specification set, like we might in `verify_metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=12&gt;\n",
       "<table id=\"table4685394776\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Name</th><th>Test</th><th>Tags</th></tr></thead>\n",
       "<thead><tr><th>str36</th><th>str27</th><th>str7</th></tr></thead>\n",
       "<tr><td>demo1.Completeness.minimum_hsc_r</td><td>$x$ &gt;= 20.0 $\\mathrm{mag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.Completeness.minimum_megacam_r</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.Completeness.minimum_megacam_u</td><td>$x$ &gt;= 20.0 $\\mathrm{mag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.Completeness.stretch_hsc_r</td><td>$x$ &gt;= 28.0 $\\mathrm{mag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.Completeness.stretch_megacam_r</td><td>$x$ &gt;= 26.0 $\\mathrm{mag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.Completeness.stretch_megacam_u</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.minimum_hsc_r</td><td>$x$ &lt;= 12.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.minimum_megacam_u</td><td>$x$ &lt;= 30.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.stretch_hsc_r</td><td>$x$ &lt;= 6.0 $\\mathrm{mmag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.stretch_megacam_r</td><td>$x$ &lt;= 10.0 $\\mathrm{mmag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.stretch_megacam_u</td><td>$x$ &lt;= 20.0 $\\mathrm{mmag}$</td><td>stretch</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<lsst.verify.specset.SpecificationSet at 0x1066adcf8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo1_specs_yaml = \"\"\"\n",
    "# Partials that define metadata queries\n",
    "# for pipeline execution contexts with\n",
    "# MegaCam r and u-band data, or HSC r-band.\n",
    "\n",
    "---\n",
    "id: \"megacam-r\"\n",
    "metadata_query:\n",
    "  camera: \"megacam\"\n",
    "  filter_name: \"r\"\n",
    "  \n",
    "---\n",
    "id: \"megacam-u\"\n",
    "metadata_query:\n",
    "  camera: \"megacam\"\n",
    "  filter_name: \"u\"\n",
    "\n",
    "---\n",
    "id: \"hsc-r\"\n",
    "metadata_query:\n",
    "  camera: \"hsc\"\n",
    "  filter_name: \"r\"\n",
    "\n",
    "# We'll also write partials for each metric,\n",
    "# that set up the basic test. Alternatively\n",
    "# we could create full specifications to\n",
    "# inherit from for each camera.\n",
    "\n",
    "---\n",
    "id: \"ZeropointRMS\"\n",
    "metric: \"demo1.ZeropointRMS\"\n",
    "threshold:\n",
    "  operator: \"<=\"\n",
    "  unit: \"mmag\"\n",
    "\n",
    "---\n",
    "id: \"Completeness\"\n",
    "metric: \"demo1.Completeness\"\n",
    "threshold:\n",
    "  operator: \">=\"\n",
    "  unit: \"mag\"\n",
    "\n",
    "# Partials to tag specifications as\n",
    "# \"minimum\" requirements or \"stretch\n",
    "# goals\"\n",
    "---\n",
    "id: \"tag-minimum\"\n",
    "tags:\n",
    "  - \"minimum\"\n",
    "\n",
    "---\n",
    "id: \"tag-stretch\"\n",
    "tags:\n",
    "  - \"stretch\"\n",
    "\n",
    "# ZeropointRMS specifications\n",
    "# tailored for each camera, in\n",
    "# minimum and stretch goal variants.\n",
    "\n",
    "---\n",
    "name: \"minimum_megacam_r\"\n",
    "base: [\"#ZeropointRMS\", \"#megacam-r\", \"#tag-minimum\"]\n",
    "threshold:\n",
    "  value: 15.0\n",
    "\n",
    "---\n",
    "name: \"stretch_megacam_r\"\n",
    "base: [\"#ZeropointRMS\", \"#megacam-r\", \"#tag-stretch\"]\n",
    "threshold:\n",
    "  value: 10.0\n",
    "\n",
    "---\n",
    "name: \"minimum_megacam_u\"\n",
    "base: [\"#ZeropointRMS\", \"#megacam-u\", \"#tag-minimum\"]\n",
    "threshold:\n",
    "  value: 30.0\n",
    "\n",
    "---\n",
    "name: \"stretch_megacam_u\"\n",
    "base: [\"#ZeropointRMS\", \"#megacam-u\", \"#tag-stretch\"]\n",
    "threshold:\n",
    "  value: 20.0\n",
    "  \n",
    "---\n",
    "name: \"minimum_hsc_r\"\n",
    "base: [\"#ZeropointRMS\", \"#hsc-r\", \"#tag-minimum\"]\n",
    "threshold:\n",
    "  value: 12.0\n",
    "\n",
    "---\n",
    "name: \"stretch_hsc_r\"\n",
    "base: [\"#ZeropointRMS\", \"#hsc-r\", \"#tag-stretch\"]\n",
    "threshold:\n",
    "  value: 6.0\n",
    " \n",
    "# Competeness specifications,\n",
    "# tailored for each camera in\n",
    "# minimum and stretch goal variants\n",
    "\n",
    "---\n",
    "name: \"minimum_megacam_r\"\n",
    "base: [\"#Completeness\", \"#megacam-r\", \"#tag-minimum\"]\n",
    "threshold:\n",
    "  value: 24.0\n",
    "\n",
    "---\n",
    "name: \"stretch_megacam_r\"\n",
    "base: [\"#Completeness\", \"#megacam-r\", \"#tag-stretch\"]\n",
    "threshold:\n",
    "  value: 26.0\n",
    "  \n",
    "---\n",
    "name: \"minimum_megacam_u\"\n",
    "base: [\"#Completeness\", \"#megacam-u\", \"#tag-minimum\"]\n",
    "threshold:\n",
    "  value: 20.0\n",
    "\n",
    "---\n",
    "name: \"stretch_megacam_u\"\n",
    "base: [\"#Completeness\", \"#megacam-u\", \"#tag-stretch\"]\n",
    "threshold:\n",
    "  value: 24.0\n",
    "  \n",
    "---\n",
    "name: \"minimum_hsc_r\"\n",
    "base: [\"#Completeness\", \"#hsc-r\", \"#tag-minimum\"]\n",
    "threshold:\n",
    "  value: 20.0\n",
    "\n",
    "---\n",
    "name: \"stretch_hsc_r\"\n",
    "base: [\"#Completeness\", \"#hsc-r\", \"#tag-stretch\"]\n",
    "threshold:\n",
    "  value: 28.0\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    # Write YAML to disk, emulating the verify_metrics package for this demo\n",
    "    specs_dirname = os.path.join(temp_dir, 'demo1')\n",
    "    os.makedirs(specs_dirname)\n",
    "    demo1_specs_path = os.path.join(specs_dirname, 'demo1.yaml')\n",
    "    with open(demo1_specs_path, mode='w') as f:\n",
    "        f.write(demo1_specs_yaml)\n",
    "\n",
    "    # Parse the YAML into a set of Specification objects\n",
    "    demo_specs = lsst.verify.SpecificationSet.load_single_package(specs_dirname)\n",
    "\n",
    "demo_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More metrics and specifications for the demo1 package\n",
    "\n",
    "All the metrics we've created have been associated with the hypothetical \"demo1\" pipeline package. Let's quickly create another set of metrics and specifications for a \"demo1\" pipeline package, which we'll use later. This is an excuse to show that metrics and specifications can be created dynamically in Python too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo2.SourceCount (dimensionless_unscaled): Number of matched sources.\n"
     ]
    }
   ],
   "source": [
    "sourcecount_metric = lsst.verify.Metric(\n",
    "    'demo2.SourceCount',\n",
    "    \"Number of matched sources.\",\n",
    "    unit=u.dimensionless_unscaled,\n",
    "    tags=['demo'])\n",
    "demo_metrics.insert(sourcecount_metric)\n",
    "\n",
    "print(demo_metrics['demo2.SourceCount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `demo1.SourceCount` is just a count; it doesn't have physical units. We designated this type of unit with Astropy's [astropy.units.dimensionless_unscaled](http://docs.astropy.org/en/stable/units/standard_units.html#the-dimensionless-unit) unit. Its string form is an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.dimensionless_unscaled == u.Unit('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create complementary specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcecount_minimum_spec = lsst.verify.ThresholdSpecification(\n",
    "    'demo2.SourceCount.minimum_cfht_r',\n",
    "    250 * u.dimensionless_unscaled,\n",
    "    '>=',\n",
    "    tags=['minimum'],\n",
    "    metadata_query={'camera': 'megacam', 'filter_name': 'r'}\n",
    ")\n",
    "demo_specs.insert(sourcecount_minimum_spec)\n",
    "\n",
    "sourcecount_stretch_spec = lsst.verify.ThresholdSpecification(\n",
    "    'demo2.SourceCount.stretch_cfht_r',\n",
    "    500 * u.dimensionless_unscaled,\n",
    "    '>=',\n",
    "    tags=['stretch'],\n",
    "    metadata_query={'camera': 'megacam', 'filter_name': 'r'}\n",
    ")\n",
    "demo_specs.insert(sourcecount_stretch_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We now have a set of metrics and specifications defined for two packages, `demo1` and `demo2`. Here are the metrics in full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=3&gt;\n",
       "<table id=\"table4685396456\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Name</th><th>Description</th><th>Units</th><th>Reference</th><th>Tags</th></tr></thead>\n",
       "<thead><tr><th>str18</th><th>str50</th><th>str15</th><th>str28</th><th>str16</th></tr></thead>\n",
       "<tr><td>demo1.Completeness</td><td>Magnitude of the catalog&apos;s 50% completeness limit.</td><td>$\\mathrm{mag}$</td><td>https://example.com/Complete</td><td>demo, photometry</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS</td><td>Photometric calibration RMS.</td><td>$\\mathrm{mmag}$</td><td>https://example.com/PhotRMS</td><td>demo, photometry</td></tr>\n",
       "<tr><td>demo2.SourceCount</td><td>Number of matched sources.</td><td>$\\mathrm{}$</td><td></td><td>demo</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<lsst.verify.metricset.MetricSet at 0x117445da0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the specifications in full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=14&gt;\n",
       "<table id=\"table4685397800\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Name</th><th>Test</th><th>Tags</th></tr></thead>\n",
       "<thead><tr><th>str36</th><th>str27</th><th>str7</th></tr></thead>\n",
       "<tr><td>demo1.Completeness.minimum_hsc_r</td><td>$x$ &gt;= 20.0 $\\mathrm{mag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.Completeness.minimum_megacam_r</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.Completeness.minimum_megacam_u</td><td>$x$ &gt;= 20.0 $\\mathrm{mag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.Completeness.stretch_hsc_r</td><td>$x$ &gt;= 28.0 $\\mathrm{mag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.Completeness.stretch_megacam_r</td><td>$x$ &gt;= 26.0 $\\mathrm{mag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.Completeness.stretch_megacam_u</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.minimum_hsc_r</td><td>$x$ &lt;= 12.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.minimum_megacam_u</td><td>$x$ &lt;= 30.0 $\\mathrm{mmag}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.stretch_hsc_r</td><td>$x$ &lt;= 6.0 $\\mathrm{mmag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.stretch_megacam_r</td><td>$x$ &lt;= 10.0 $\\mathrm{mmag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo1.ZeropointRMS.stretch_megacam_u</td><td>$x$ &lt;= 20.0 $\\mathrm{mmag}$</td><td>stretch</td></tr>\n",
       "<tr><td>demo2.SourceCount.minimum_cfht_r</td><td>$x$ &gt;= 250.0 $\\mathrm{}$</td><td>minimum</td></tr>\n",
       "<tr><td>demo2.SourceCount.stretch_cfht_r</td><td>$x$ &gt;= 500.0 $\\mathrm{}$</td><td>stretch</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<lsst.verify.specset.SpecificationSet at 0x1066adcf8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these examples are contrived for this tutorial. Normally metrics and specifications aren't defined in notebooks or code, but with a pull request to the verify_metrics GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making measurements\n",
    "\n",
    "Now that we've defined metrics, we can measure them. Measurements happen in Pipelines code, either within regular Tasks, or in dedicated afterburner Tasks.\n",
    "\n",
    "The Verification Framework provides two patterns for making measurements: either using the full measurement API, or a more lightweight capture of measurement quantities. For the `demo1` package we'll use the more comprehensive approach, and make lightweight measurements for the `demo2` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring ZeropointRMS\n",
    "\n",
    "In our Task, we might have arrays of matched photometry and catalogs stars with known photometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catalog_mags = np.random.uniform(18, 26, size=100)*u.mag\n",
    "\n",
    "obs_mags = catalog_mags - 25*u.mag + np.random.normal(scale=12.0, size=100)*u.mmag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these the task might estimate a zeropoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zp = np.median(catalog_mags - obs_mags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a scatter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zp_rms = np.std(catalog_mags - obs_mags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`zp_rms` is a measurement of the `demo1.ZeropointRMS` metric that we'd like to capture. Let's create a `lsst.verify.Measurement` object to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zp_meas = lsst.verify.Measurement('demo1.ZeropointRMS', zp_rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've captured the measurement, but there's more information that we be useful for later understanding the measurement. These additional data are called measurement *extras*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zp_meas.extras['zp'] = lsst.verify.Datum(zp, label=\"m_0\",\n",
    "                                         description=\"Estimated zeropoint.\")\n",
    "zp_meas.extras['catalog_mags'] = lsst.verify.Datum(catalog_mags, label=\"m_cat\",\n",
    "                                                   description=\"Catalog magnitudes.\")\n",
    "zp_meas.extras['obs_mags'] = lsst.verify.Datum(obs_mags, label=\"m_obs\",\n",
    "                                               description=\"Instrument magnitudes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Datum` acts as a wrapper as information, like Astropy quantities, that adds plotting labels and descriptions to help document our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Task, we might want to add annotations about the Task's configuration. These annotations will be added to the metadata of the pipeline execution. For example, this is an annotation of the function used to estimate the RMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zp_meas.notes['estimator'] = 'numpy.std'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Completeness\n",
    "\n",
    "Our task also measures photometric completeness. Let's making another `Measurement` to record this metric measurement, along with extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's a mock dataset\n",
    "mag_grid = np.linspace(22, 28, num=50, endpoint=True)\n",
    "c_percent = 1. / np.cosh((mag_grid - mag_grid.min()) / 2.) * 100.\n",
    "\n",
    "# Make the measurement\n",
    "completeness_mag = np.interp(50.0, c_percent[::-1], mag_grid[::-1]) * u.mag\n",
    "\n",
    "# Package the measurement\n",
    "completeness_meas = lsst.verify.Measurement(\n",
    "    'demo1.Completeness',\n",
    "    completeness_mag,\n",
    ")\n",
    "completeness_meas.extras['mag_grid'] = lsst.verify.Datum(\n",
    "    mag_grid*u.mag,\n",
    "    label=\"m\",\n",
    "    description=\"Magnitude\")\n",
    "completeness_meas.extras['c_frac'] = lsst.verify.Datum(\n",
    "    c_percent*u.percent,\n",
    "    label=\"C\",\n",
    "    description=\"Photometric catalog completeness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging measurements in a Verification Job\n",
    "\n",
    "In the Verification Framework, a \"job\" is a run of pipeline that produces metric measurements. The `lsst.verify.Job` class allows us to package several measurements from the pipeline run. With a `Job` object, we can then analyze the measurements, save verification datasets to disk, and dispatch datasets to the SQUASH database.\n",
    "\n",
    "Normally when we create a `Job` object from scratch we seed it with the metrics and specifications defined in the `verify_metrics` repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job = lsst.verify.Job.load_metrics_package()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we created ad hoc metrics and specifications outside of `verify_metrics`. We can add those to the `job`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job.metrics.update(demo_metrics)\n",
    "job.specs.update(demo_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job.measurements.insert(zp_meas)\n",
    "job.measurements.insert(completeness_meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline Tasks that is making this Job knows about the camera and filter of the dataset. The Task code can record this metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job.meta.update({'camera': 'megacam', 'filter_name': 'r'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job metadata is a `dict`-like mapping. Here's the full set of metadata recorded for the `job`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"camera\": \"megacam\",\n",
      "    \"demo1.ZeropointRMS.estimator\": \"numpy.std\",\n",
      "    \"filter_name\": \"r\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(job.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the `camera` and `filter_name` is present, but so is the `estimator` annotation that we attached to the `demo1.ZeropointRMS` measurement. Measurement annotations are automatically included in a Job's metadata, but keys are prefixed with the measurement's metric name. Specification `metadata_query` definitions can act on both job and measurement-level metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before a Task exits, it should write the verification Job dataset to disk. Serialization to disk is a temporary shim until `Job` datasets can be persisted through the Butler.\n",
    "\n",
    "The native serialization format of the Verification Framework is JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job.write('demo1.verify.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making lightweight quantity-only measurements with output_quantities()\n",
    "\n",
    "`lsst.verify.Measurement` and `lsst.verify.Job` classes are necessary for producing rich job datasets (for example, associating extras with measurements. Many Tasks, though, won't need this functionality. A Task might record a measurement as an astropy quantity and persist that measurement with as little overhead as possible. The `lsst.verify.output_quantities` function enables this usecase.\n",
    "\n",
    "First, a Task will create a dictionary to collect measurements throughout the lifetime of the Task's execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo2_measurements = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the task measures the `demo2.SourceCount` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo2_measurements['demo2.SourceCount'] = 350*u.dimensionless_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measurements are *always* Astropy quantities.\n",
    "\n",
    "Finally, before the Task returns, it can output measurements to disk. The default filename format for the Verification job dataset file is `{package}.verify.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demo2.verify.json'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsst.verify.output_quantities('demo2', demo2_measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing verification jobs\n",
    "\n",
    "Our hypothetical pipeline has produced measurements for two packages: `demo1` and `demo2`. These measurements are persisted to `demo1.verify.json` and `demo2.verify.json` files on disk. Now we'd like to gather these measurements and either submit them to the SQUASH dashboard, or collate the measurements for local analysis.\n",
    "\n",
    "The `dispatch_verify.py` tool lets us do this. For this demo we won't upload measurements to SQUASH. Instead we will combine the mesurements into one JSON file, and also add metadata about the versions of the pipeline packages that produced the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify.bin.dispatchverify.main INFO: Loading demo1.verify.json\n",
      "verify.bin.dispatchverify.main INFO: Loading demo2.verify.json\n",
      "verify.bin.dispatchverify.main INFO: Merging verification Job JSON.\n",
      "verify.bin.dispatchverify.main INFO: Refreshing metric definitions from verify_metrics\n",
      "verify.bin.dispatchverify.main INFO: Writing Job JSON to demo.verify.json.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export DYLD_LIBRARY_PATH=$LSST_LIBRARY_PATH\n",
    "dispatch_verify.py --test --ignore-lsstsw --write demo.verify.json demo1.verify.json demo2.verify.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flags used here are:\n",
    "    \n",
    "- `--test`: prevents `dispatch_verify.py` from attempting to upload to the SQUASH service.\n",
    "- `--ignore-lsstsw`: since the `$LSSTSW` environment variable may not be available in this notebook context, we'll avoid scraping it for information (such as Git commits and branches of packages included in the Pipeline stack).\n",
    "- `--write demo.verify.json`: Write the merged job dataset to `demo.verify.json`.\n",
    "- `demo1.verify.json` and `demo2.verify.json` are inputs, as position arguments, pointing to the job JSON files that we created earlier with metric measurements.\n",
    "\n",
    "See `dispatch_verify.py --help` for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analyze verification results locally\n",
    "\n",
    "For code development, it's convenient to look at the results of verification measurements locally, rather than in SQUASH. The Verification Framework is designed for this workflow, with special affordances for Jupyter Notebook users.\n",
    "\n",
    "The collated measurement dataset produced by `dispatch_verify.py` earlier is in the file `demo.verify.json`. Let's open this dataset using the `Job.deserialize` class method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('demo.verify.json') as f:\n",
    "    job = lsst.verify.Job.deserialize(**json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a job dataset, we can make a report that summarizes the pass/fail status of specifications that have corresponding measurement. Reports, `lsst.verify.Report` instances, are thin wrappers around Astropy Tables, and look great in Jupyter Notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=6&gt;\n",
       "<table id=\"table4685321664-865312\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Status</th><th>Specification</th><th>Measurement</th><th>Test</th><th>Metric Tags</th><th>Spec. Tags</th></tr></thead>\n",
       "<tr><td>✅</td><td>demo1.Completeness.minimum_megacam_r</td><td>24.6 $\\mathrm{mag}$</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>❌</td><td>demo1.Completeness.stretch_megacam_r</td><td>24.6 $\\mathrm{mag}$</td><td>$x$ &gt;= 26.0 $\\mathrm{mag}$</td><td>demo, photometry</td><td>stretch</td></tr>\n",
       "<tr><td>✅</td><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>❌</td><td>demo1.ZeropointRMS.stretch_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 10.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>stretch</td></tr>\n",
       "<tr><td>✅</td><td>demo2.SourceCount.minimum_cfht_r</td><td>350.0 $\\mathrm{}$</td><td>$x$ &gt;= 250.0 $\\mathrm{}$</td><td>demo</td><td>minimum</td></tr>\n",
       "<tr><td>❌</td><td>demo2.SourceCount.stretch_cfht_r</td><td>350.0 $\\mathrm{}$</td><td>$x$ &gt;= 500.0 $\\mathrm{}$</td><td>demo</td><td>stretch</td></tr>\n",
       "</table><style>table.dataTable {clear: both; width: auto !important; margin: 0 !important;}\n",
       ".dataTables_info, .dataTables_length, .dataTables_filter, .dataTables_paginate{\n",
       "display: inline-block; margin-right: 1em; }\n",
       ".paginate_button { margin-right: 5px; }\n",
       "</style>\n",
       "<script>\n",
       "require.config({paths: {\n",
       "    datatables: 'https://cdn.datatables.net/1.10.9/js/jquery.dataTables.min'\n",
       "}});\n",
       "require([\"datatables\"], function(){\n",
       "    console.log(\"$('#table4685321664-865312').dataTable()\");\n",
       "    $('#table4685321664-865312').dataTable({\n",
       "        \"order\": [],\n",
       "        \"iDisplayLength\": 50,\n",
       "        \"aLengthMenu\": [[10, 25, 50, 100, 500, 1000, -1], [10, 25, 50, 100, 500, 1000, 'All']],\n",
       "        \"pagingType\": \"full_numbers\"\n",
       "    });\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.report().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the report only shows specification tests that are relevant to the measurements. Recall that the job metadata indicates these measurements are with CFHT/MegaCam in the $r$-band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"camera\": \"megacam\",\n",
      "    \"demo1.ZeropointRMS.estimator\": \"numpy.std\",\n",
      "    \"filter_name\": \"r\",\n",
      "    \"packages\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(job.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus all the specifications having to do with HSC or the $u$-band aren't tested because those tests are meaningless with the current measurements.\n",
    "\n",
    "When there are many measurements and specifications, you might be more interesting in producing reports around specific topics. Such tailored reports can be made by passing arguments to the `Job.report` method.\n",
    "\n",
    "For example, this is a report listing only `demo1` package metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=4&gt;\n",
       "<table id=\"table4692163608-610916\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Status</th><th>Specification</th><th>Measurement</th><th>Test</th><th>Metric Tags</th><th>Spec. Tags</th></tr></thead>\n",
       "<tr><td>✅</td><td>demo1.Completeness.minimum_megacam_r</td><td>24.6 $\\mathrm{mag}$</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>❌</td><td>demo1.Completeness.stretch_megacam_r</td><td>24.6 $\\mathrm{mag}$</td><td>$x$ &gt;= 26.0 $\\mathrm{mag}$</td><td>demo, photometry</td><td>stretch</td></tr>\n",
       "<tr><td>✅</td><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>❌</td><td>demo1.ZeropointRMS.stretch_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 10.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>stretch</td></tr>\n",
       "</table><style>table.dataTable {clear: both; width: auto !important; margin: 0 !important;}\n",
       ".dataTables_info, .dataTables_length, .dataTables_filter, .dataTables_paginate{\n",
       "display: inline-block; margin-right: 1em; }\n",
       ".paginate_button { margin-right: 5px; }\n",
       "</style>\n",
       "<script>\n",
       "require.config({paths: {\n",
       "    datatables: 'https://cdn.datatables.net/1.10.9/js/jquery.dataTables.min'\n",
       "}});\n",
       "require([\"datatables\"], function(){\n",
       "    console.log(\"$('#table4692163608-610916').dataTable()\");\n",
       "    $('#table4692163608-610916').dataTable({\n",
       "        \"order\": [],\n",
       "        \"iDisplayLength\": 50,\n",
       "        \"aLengthMenu\": [[10, 25, 50, 100, 500, 1000, -1], [10, 25, 50, 100, 500, 1000, 'All']],\n",
       "        \"pagingType\": \"full_numbers\"\n",
       "    });\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.report(name='demo1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this report shows results for the `demo1.ZeropointRMS` metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=2&gt;\n",
       "<table id=\"table4692163608-539416\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Status</th><th>Specification</th><th>Measurement</th><th>Test</th><th>Metric Tags</th><th>Spec. Tags</th></tr></thead>\n",
       "<tr><td>✅</td><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>❌</td><td>demo1.ZeropointRMS.stretch_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 10.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>stretch</td></tr>\n",
       "</table><style>table.dataTable {clear: both; width: auto !important; margin: 0 !important;}\n",
       ".dataTables_info, .dataTables_length, .dataTables_filter, .dataTables_paginate{\n",
       "display: inline-block; margin-right: 1em; }\n",
       ".paginate_button { margin-right: 5px; }\n",
       "</style>\n",
       "<script>\n",
       "require.config({paths: {\n",
       "    datatables: 'https://cdn.datatables.net/1.10.9/js/jquery.dataTables.min'\n",
       "}});\n",
       "require([\"datatables\"], function(){\n",
       "    console.log(\"$('#table4692163608-539416').dataTable()\");\n",
       "    $('#table4692163608-539416').dataTable({\n",
       "        \"order\": [],\n",
       "        \"iDisplayLength\": 50,\n",
       "        \"aLengthMenu\": [[10, 25, 50, 100, 500, 1000, -1], [10, 25, 50, 100, 500, 1000, 'All']],\n",
       "        \"pagingType\": \"full_numbers\"\n",
       "    });\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.report(name='demo1.ZeropointRMS').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we added tags to the specifications to designate `minimum` and `stretch` goals, as in seen in the `demo1.ZeropointRMS.minimum_megacam_r` specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.specs['demo1.ZeropointRMS.minimum_megacam_r'].tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tailor the report to show tests only against these `minimum` specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=3&gt;\n",
       "<table id=\"table4692160640-938091\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Status</th><th>Specification</th><th>Measurement</th><th>Test</th><th>Metric Tags</th><th>Spec. Tags</th></tr></thead>\n",
       "<tr><td>✅</td><td>demo1.Completeness.minimum_megacam_r</td><td>24.6 $\\mathrm{mag}$</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>✅</td><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>✅</td><td>demo2.SourceCount.minimum_cfht_r</td><td>350.0 $\\mathrm{}$</td><td>$x$ &gt;= 250.0 $\\mathrm{}$</td><td>demo</td><td>minimum</td></tr>\n",
       "</table><style>table.dataTable {clear: both; width: auto !important; margin: 0 !important;}\n",
       ".dataTables_info, .dataTables_length, .dataTables_filter, .dataTables_paginate{\n",
       "display: inline-block; margin-right: 1em; }\n",
       ".paginate_button { margin-right: 5px; }\n",
       "</style>\n",
       "<script>\n",
       "require.config({paths: {\n",
       "    datatables: 'https://cdn.datatables.net/1.10.9/js/jquery.dataTables.min'\n",
       "}});\n",
       "require([\"datatables\"], function(){\n",
       "    console.log(\"$('#table4692160640-938091').dataTable()\");\n",
       "    $('#table4692160640-938091').dataTable({\n",
       "        \"order\": [],\n",
       "        \"iDisplayLength\": 50,\n",
       "        \"aLengthMenu\": [[10, 25, 50, 100, 500, 1000, -1], [10, 25, 50, 100, 500, 1000, 'All']],\n",
       "        \"pagingType\": \"full_numbers\"\n",
       "    });\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.report(spec_tags=['minimum']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `spec_tags` argument takes a sequence of tags. Each tag is treated as an `AND` filter with the others. For example, there are no specifications that are both `minimum` and `stretch`, so the report is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=0&gt;\n",
       "<table id=\"table4692163664-289402\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Status</th><th>Specification</th><th>Measurement</th><th>Test</th><th>Metric Tags</th><th>Spec. Tags</th></tr></thead>\n",
       "</table><style>table.dataTable {clear: both; width: auto !important; margin: 0 !important;}\n",
       ".dataTables_info, .dataTables_length, .dataTables_filter, .dataTables_paginate{\n",
       "display: inline-block; margin-right: 1em; }\n",
       ".paginate_button { margin-right: 5px; }\n",
       "</style>\n",
       "<script>\n",
       "require.config({paths: {\n",
       "    datatables: 'https://cdn.datatables.net/1.10.9/js/jquery.dataTables.min'\n",
       "}});\n",
       "require([\"datatables\"], function(){\n",
       "    console.log(\"$('#table4692163664-289402').dataTable()\");\n",
       "    $('#table4692163664-289402').dataTable({\n",
       "        \"order\": [],\n",
       "        \"iDisplayLength\": 50,\n",
       "        \"aLengthMenu\": [[10, 25, 50, 100, 500, 1000, -1], [10, 25, 50, 100, 500, 1000, 'All']],\n",
       "        \"pagingType\": \"full_numbers\"\n",
       "    });\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.report(spec_tags=['minimum', 'stretch']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to specification tags, on can filter by metric tags by setting teh `metric_tags` argument.\n",
    "\n",
    "Finally, these filters can be combined. For example, this report summarizes specification tests for metrics from the `demo1` package against `minimum` goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=2&gt;\n",
       "<table id=\"table4692163944-90239\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Status</th><th>Specification</th><th>Measurement</th><th>Test</th><th>Metric Tags</th><th>Spec. Tags</th></tr></thead>\n",
       "<tr><td>✅</td><td>demo1.Completeness.minimum_megacam_r</td><td>24.6 $\\mathrm{mag}$</td><td>$x$ &gt;= 24.0 $\\mathrm{mag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "<tr><td>✅</td><td>demo1.ZeropointRMS.minimum_megacam_r</td><td>11.2 $\\mathrm{mmag}$</td><td>$x$ &lt;= 15.0 $\\mathrm{mmag}$</td><td>demo, photometry</td><td>minimum</td></tr>\n",
       "</table><style>table.dataTable {clear: both; width: auto !important; margin: 0 !important;}\n",
       ".dataTables_info, .dataTables_length, .dataTables_filter, .dataTables_paginate{\n",
       "display: inline-block; margin-right: 1em; }\n",
       ".paginate_button { margin-right: 5px; }\n",
       "</style>\n",
       "<script>\n",
       "require.config({paths: {\n",
       "    datatables: 'https://cdn.datatables.net/1.10.9/js/jquery.dataTables.min'\n",
       "}});\n",
       "require([\"datatables\"], function(){\n",
       "    console.log(\"$('#table4692163944-90239').dataTable()\");\n",
       "    $('#table4692163944-90239').dataTable({\n",
       "        \"order\": [],\n",
       "        \"iDisplayLength\": 50,\n",
       "        \"aLengthMenu\": [[10, 25, 50, 100, 500, 1000, -1], [10, 25, 50, 100, 500, 1000, 'All']],\n",
       "        \"pagingType\": \"full_numbers\"\n",
       "    });\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.report(name='demo1', spec_tags=['minimum']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data behind the measurements\n",
    "\n",
    "Besides reports of specifications that were met or failed during a job, we're also interested in the context of the measurements. What was the distribution of points? Where were sources on the detector? These questions cannot be answered by metrics, which are scalars by definitions. But they might be answered by the *blob* datasets that accompany measurements.\n",
    "\n",
    "Recall that during the `demo1` measurements we added \"extras,\" consisting of raw arrays of magnitudes, as well as the fitted zeropoint. We can access these blobs datasets and make plots for deeper investigations.\n",
    "\n",
    "First, we access the `demo1.ZeropointRMS` metric measurement in the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = job.measurements['demo1.ZeropointRMS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra data associated with the measurement are stored as key-value items in the measurement's `extras` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obs_mags', 'zp', 'catalog_mags']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m.extras.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we'll use Bokeh to make interactive plots with this data. Often it's easiest to pack a Pandas DataFrame for plotting with Bokeh. We'll make the DataFrame from the Astropy `Quantity` array, accessed from the `quantity` attributes of each item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pandas.DataFrame({\"obs_mags\": m.extras['obs_mags'].quantity,\n",
    "                       \"catalog_mags\": m.extras['catalog_mags'].quantity,\n",
    "                       \"delta_mags\": m.extras['catalog_mags'].quantity - m.extras['obs_mags'].quantity})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These items, `obs_mags` and `catalog_mags`, are `lsst.verify.Datum` instances. `Datum` objects allow us to pack information with data, such as plot labels. Here we'll use that metadata to build plot labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot of observed vs. catalog stellar photometry\n",
    "p = Scatter(df, x='obs_mags', y='catalog_mags',\n",
    "            title=\"Zeropoint stellar sample\",\n",
    "            xlabel=\"{0.label} [{0.unit}]\".format(m.extras['obs_mags']),\n",
    "            ylabel=\"{0.label} [{0.unit}]\".format(m.extras['catalog_mags']),\n",
    "            plot_width=350, plot_height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram of zeropoint estimates from individual matched stars.\n",
    "# We're not using the Histogram Bokeh chart for some extra control.\n",
    "hist_counts, hist_edges = np.histogram(df['delta_mags'], bins=10)\n",
    "h = figure(tools=\"xpan, xwheel_zoom, reset\",\n",
    "           active_scroll=\"xwheel_zoom\",\n",
    "           y_range=(0, hist_counts.max()+2),\n",
    "           y_axis_label=\"Count\",\n",
    "           x_axis_label=\"{0.label} - {1.label} [{0.unit}]\".format(m.extras['obs_mags'], m.extras['catalog_mags']),\n",
    "           plot_width=350, plot_height=350)\n",
    "# Draw histogram edges on the figure\n",
    "h.quad(bottom=0,\n",
    "       left=hist_edges[:-1],\n",
    "       right=hist_edges[1:],\n",
    "       top=hist_counts,\n",
    "       color=\"lightblue\",\n",
    "       line_color=\"#3A5785\")\n",
    "# Line at zeropoint estimate\n",
    "span = Span(location=m.extras['zp'].quantity.value,\n",
    "            dimension='height', line_color=\"black\",\n",
    "            line_dash='dashed', line_width=3)\n",
    "h.add_layout(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"bk-plotdiv\" id=\"1f651d4d-52ce-4eba-813d-e1d37cf70529\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = false;\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        var el = document.getElementById(\"1f651d4d-52ce-4eba-813d-e1d37cf70529\");\n",
       "        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }if ((window.Jupyter !== undefined) && Jupyter.notebook.kernel) {\n",
       "      comm_manager = Jupyter.notebook.kernel.comm_manager\n",
       "      comm_manager.register_target(\"3db116e8-b3b2-4ff3-a261-eb5792575023\", function () {});\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"1f651d4d-52ce-4eba-813d-e1d37cf70529\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1f651d4d-52ce-4eba-813d-e1d37cf70529' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        (function() {\n",
       "          var fn = function() {\n",
       "            var docs_json = {\"b35a5cbe-3e75-48ce-b7fa-3a1fadd249d4\":{\"roots\":{\"references\":[{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":{\"id\":\"a3fb5fec-8253-4c77-99f9-f1f10302626e\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"8cda94be-c78a-4895-978a-e0a077062d88\",\"type\":\"PanTool\"},{\"id\":\"a3fb5fec-8253-4c77-99f9-f1f10302626e\",\"type\":\"WheelZoomTool\"},{\"id\":\"90d3e7c8-0a96-4563-a70e-7712d596a28f\",\"type\":\"ResetTool\"}]},\"id\":\"8f4290fc-4d08-40af-8d02-e0ac42200866\",\"type\":\"Toolbar\"},{\"attributes\":{\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"4070654b-1511-4b66-9a68-95f2521617e9\",\"type\":\"ResetTool\"},{\"attributes\":{\"callback\":null,\"end\":1.7211044031083498,\"start\":-7.455487826731361},\"id\":\"dc809927-3302-4aad-bc67-342de2a07540\",\"type\":\"Range1d\"},{\"attributes\":{\"dimension\":\"height\",\"line_dash\":[6],\"line_width\":{\"value\":3},\"location\":24.998261573478807,\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"0413fe5e-4d5d-47f0-8906-1f62a1d2ad77\",\"type\":\"Span\"},{\"attributes\":{\"callback\":null,\"end\":28},\"id\":\"3bcf6f7a-3327-4f36-bc28-edecd7979031\",\"type\":\"Range1d\"},{\"attributes\":{\"axis_label\":\"m_obs [mag]\",\"formatter\":{\"id\":\"12b8bcc9-abb3-49c8-86e6-a2a6b2e8ea18\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b2258f67-8427-4c1e-8fc0-d0027206a4c7\",\"type\":\"BasicTicker\"}},\"id\":\"b48d75dd-e7db-40ac-8927-14b909acac1c\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"7ea125e9-3437-413b-b51d-ed540d0e93f7\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"b2258f67-8427-4c1e-8fc0-d0027206a4c7\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimensions\":\"width\",\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"a3fb5fec-8253-4c77-99f9-f1f10302626e\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"below\":[{\"id\":\"5639f8a4-7f19-4a50-8058-8d6017de00da\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"059d093b-ce54-4d4e-b508-4b959626aa17\",\"type\":\"LinearAxis\"}],\"plot_height\":350,\"plot_width\":350,\"renderers\":[{\"id\":\"5639f8a4-7f19-4a50-8058-8d6017de00da\",\"type\":\"LinearAxis\"},{\"id\":\"11390e2a-9c15-4608-8f7e-b3a09788d222\",\"type\":\"Grid\"},{\"id\":\"059d093b-ce54-4d4e-b508-4b959626aa17\",\"type\":\"LinearAxis\"},{\"id\":\"b6918fce-8674-4eac-a460-070a4c501bdd\",\"type\":\"Grid\"},{\"id\":\"b3fb3eec-510b-4910-a1cb-d08b4b0282c4\",\"type\":\"GlyphRenderer\"},{\"id\":\"0413fe5e-4d5d-47f0-8906-1f62a1d2ad77\",\"type\":\"Span\"}],\"title\":{\"id\":\"e25fce82-3afc-46a6-afa5-5daf3a681bcc\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"32552d0e-9c60-4694-807c-36d0d5ad3478\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"8f4290fc-4d08-40af-8d02-e0ac42200866\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"39a9760f-ac46-4efb-aa9a-9d80558b4ba8\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"3bcf6f7a-3327-4f36-bc28-edecd7979031\",\"type\":\"Range1d\"}},\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"children\":[{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"},{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"58dcb1ab-b444-4dfc-83d6-b84acc86cb36\",\"type\":\"Row\"},{\"attributes\":{\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b2258f67-8427-4c1e-8fc0-d0027206a4c7\",\"type\":\"BasicTicker\"}},\"id\":\"42e6ca61-9d97-48c6-99c3-e1cc50cfaf3e\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"04b8552b-7bab-4d46-bd50-a515b5bc131c\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"04b8552b-7bab-4d46-bd50-a515b5bc131c\",\"type\":\"BasicTicker\"}},\"id\":\"4432d387-933f-4860-9285-9867d82eadd4\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"b41f77a7-035e-4d36-9c44-e2f3dc943e5e\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"740eebc9-f7b5-4342-8020-40d5b4500080\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null},\"id\":\"39a9760f-ac46-4efb-aa9a-9d80558b4ba8\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"end\":26.71247097547604,\"start\":17.554192965632954},\"id\":\"d58e6378-56dd-41af-9b96-eb3dfb4a4cec\",\"type\":\"Range1d\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"8a7729d3-9c4c-49ea-942b-ab06f350d059\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"740eebc9-f7b5-4342-8020-40d5b4500080\",\"type\":\"BasicTicker\"}},\"id\":\"11390e2a-9c15-4608-8f7e-b3a09788d222\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"16e57199-11d7-4a90-95d9-995a82d53ea4\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimensions\":\"width\",\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"8cda94be-c78a-4895-978a-e0a077062d88\",\"type\":\"PanTool\"},{\"attributes\":{\"axis_label\":\"Count\",\"formatter\":{\"id\":\"a384febe-8247-4662-b7ec-37a9c1134805\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"16e57199-11d7-4a90-95d9-995a82d53ea4\",\"type\":\"BasicTicker\"}},\"id\":\"059d093b-ce54-4d4e-b508-4b959626aa17\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_color\":{\"value\":\"lightblue\"},\"left\":{\"field\":\"left\"},\"line_color\":{\"value\":\"#3A5785\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"65be9aa4-85c6-4d3b-98ea-272e32ad6c28\",\"type\":\"Quad\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"16569e95-6faf-4829-891e-d1c84493d837\",\"type\":\"Quad\"},{\"attributes\":{\"plot\":null,\"text\":\"Zeropoint stellar sample\"},\"id\":\"df4a425d-d06f-496c-9f30-b9004971b4c1\",\"type\":\"Title\"},{\"attributes\":{\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"dac4d1d2-6ca9-4889-9972-3455884ae92e\",\"type\":\"HelpTool\"},{\"attributes\":{\"below\":[{\"id\":\"b48d75dd-e7db-40ac-8927-14b909acac1c\",\"type\":\"LinearAxis\"}],\"css_classes\":null,\"left\":[{\"id\":\"d4eaffe1-a785-47cf-9afd-929fa571be01\",\"type\":\"LinearAxis\"}],\"plot_height\":350,\"plot_width\":350,\"renderers\":[{\"id\":\"8a7729d3-9c4c-49ea-942b-ab06f350d059\",\"type\":\"BoxAnnotation\"},{\"id\":\"acf3a4cc-f436-4fc0-bd51-bee76e239b2e\",\"type\":\"GlyphRenderer\"},{\"id\":\"f8c8a56a-4da5-4fa7-a358-77c18172e0b4\",\"type\":\"Legend\"},{\"id\":\"b48d75dd-e7db-40ac-8927-14b909acac1c\",\"type\":\"LinearAxis\"},{\"id\":\"d4eaffe1-a785-47cf-9afd-929fa571be01\",\"type\":\"LinearAxis\"},{\"id\":\"42e6ca61-9d97-48c6-99c3-e1cc50cfaf3e\",\"type\":\"Grid\"},{\"id\":\"4432d387-933f-4860-9285-9867d82eadd4\",\"type\":\"Grid\"}],\"title\":{\"id\":\"df4a425d-d06f-496c-9f30-b9004971b4c1\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"db755a54-7885-4b05-be3b-15b56f7f1feb\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"84f77da6-1fe7-44fc-a393-155b3a5086f9\",\"type\":\"Toolbar\"},\"x_mapper_type\":\"auto\",\"x_range\":{\"id\":\"dc809927-3302-4aad-bc67-342de2a07540\",\"type\":\"Range1d\"},\"y_mapper_type\":\"auto\",\"y_range\":{\"id\":\"d58e6378-56dd-41af-9b96-eb3dfb4a4cec\",\"type\":\"Range1d\"}},\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"32552d0e-9c60-4694-807c-36d0d5ad3478\",\"type\":\"ToolEvents\"},{\"attributes\":{\"location\":\"top_left\",\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"f8c8a56a-4da5-4fa7-a358-77c18172e0b4\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"4b4195d8-8c40-492c-b7eb-2cceba724000\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"e25fce82-3afc-46a6-afa5-5daf3a681bcc\",\"type\":\"Title\"},{\"attributes\":{\"axis_label\":\"m_obs - m_cat [mag]\",\"formatter\":{\"id\":\"b41f77a7-035e-4d36-9c44-e2f3dc943e5e\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"740eebc9-f7b5-4342-8020-40d5b4500080\",\"type\":\"BasicTicker\"}},\"id\":\"5639f8a4-7f19-4a50-8058-8d6017de00da\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"e9253670-7df8-4d78-9647-b8ec106b6b8a\",\"type\":\"PanTool\"},{\"attributes\":{\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"c5d27f91-be5a-4287-bcf1-48591507bdd4\",\"type\":\"SaveTool\"},{\"attributes\":{\"data_source\":{\"id\":\"e5733881-dc18-4582-808c-754f7fd2998a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"65be9aa4-85c6-4d3b-98ea-272e32ad6c28\",\"type\":\"Quad\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"16569e95-6faf-4829-891e-d1c84493d837\",\"type\":\"Quad\"},\"selection_glyph\":null},\"id\":\"b3fb3eec-510b-4910-a1cb-d08b4b0282c4\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"12b8bcc9-abb3-49c8-86e6-a2a6b2e8ea18\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x_values\",\"y_values\"],\"data\":{\"chart_index\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],\"x_values\":{\"__ndarray__\":\"vq+mb8dGF8CZbWvJGmz+v/tF6l4Bn6C/GDdu90m3BsBXIQuVAXwawDo+Fk2aWOQ/qC9Kr1nDGsC6Wg+pGejmP1VKcXgqWhrADlNpcfFbD8AaiuKjy3ETwBzNQ7pqcRXAzP5eChtnGcDg1X1v6eMUwO2nr60HWA3AcToGC3WMC8CTSZsu20L3v/lHWeVUUxDAZbcHEPDuFsD/hqCpvgkTwLpsC5x5i/q/tKM0yqwG4T+JC8OeWnkTwI1cyx26pRHAfyUyiI2gGMBxbVdWuWUJwFe5mSrnOBfAXFQCnXWQ5D/RCUtx92IUwI707I1Yrg7AcrgA6E7PDsBcYcRCDXcFwMEwpsJlSfC/LN+G35JzEsBnhUNLwWgBwLRyoc96fxHAebcRUZF8GcCtnnj1olcDwLdHZ1iTVeY/Xlcn2sIu2b8kMCaIfykYwBfstVBlDvm/pbRJFx8IAMBppvnhslz2v3s9+/56RtM/GBKHiU7tvL+RpHKMv9/gP47L0+eRuhLAkxFJmD8m87/zJJHojz8TwHpf72z6Uw7AI/Q3oGGhCsBvLyPO/jwXwEoubsirZBHAJ2SON31BF8APeWNLBDIRwJzjKAgAmRLAywOWgron/L/BgiYU0pAHwN2O68+7mu4/JUep3sGI8781+w/c9w0HwA+zMWhVKd6/sD5n4vUD47+8FUU6w1UKwMJfRjXUidC/Dv6qhLbvF8CYYPaiWKoCwKnVSOgmwvC/P0/22s7sEsBjOAeZZWUWwA1ajGsDhhrACHpp3fqhEsBZWsstooIYwGq9l6/BfgPAJXgeW1Iv6r/VOniW2gcFwCRYdgD3LQ3AsZFNxMF7xL9icuYss2UDwFQm7czxjPi/Wog7TZrkBsBJxTw4v3/yv9qKAoL2hgvA7l+sI3l7+L9WiLOj4vIVwFzFLzfG8OY/TsN1t/4JBsDCJ3tTdJ0LwI3GluaOJAzAa89r3AaI6j+3fOa4uiIXwOWmfyiClv6/9AOaIy2B6j8rAJDpnHcCwCyrekR8xgLASDtPgesVEsDUbaak6i8YwDx0RRDXmhXAdbmZFWEAEMA=\",\"dtype\":\"float64\",\"shape\":[100]},\"y_values\":{\"__ndarray__\":\"KX+0c4YrM0CJ03krxRQ3QBbsf9tW+DhABZZ/Y6EsNkDLqCSN0F8yQB+CucdFoDlAaGXJ/z9RMkDsS/9mebc5QLZr4FODaTJAcGHRmZ0XNUAVrVO6CSY0QFFxmIBTojNAVzu93UulMkCVk/tOjcMzQBWUGZqXUzVAE6dN4i2PNUDvkPQTHog3QC2Zuj8z6TRAsQKjt1xBM0BK78lHJzw0QKgbPwsWWjdAQsVAhhKJOUADsNAqCSI0QCVN8L9jkzRA35G+0zDYMkCSRe5i4tM1QKqm1+hPLjNAEAWwFdqpOUA0pMzGkugzQGxO2AYYKTVAasc0akwkNUBvN3DiWk82QDaJBUu1/DdAdI4JP0ljNEB4wa06d9I2QJjoXDjSnTRA0sdSp8qgMkBHZ5YifZE2QG6FhDLYsDlA4jEuYtGaOEAijCB3N/MyQOBN1dYEajdATp65kLIAN0AUfis4pJ83QKtw3jkLTjlAXx2y0g3mOEC8y8djnYM5QC+ehkrTSzRAg3bIGdDNN0D1jDkgPzA0QMgnYS7pKzVALLP4i2CrNUC8pjsEbTEzQOmJZOCFqDRAL2z/1kIwM0C34+k1WKw0QNltJGv+VTRAu7iSRc0/N0Dfjsz/NQw2QIKwwBYE8zlAcrt4++vBN0AaIhTYhxg2QLdmg1wyhjhAcpapc5tsOEDdHz8y/LQ1QJeb1BwIvThA05eU69gGM0C8i8669ak2QI4FO+6s9DdAKw2RtitGNECEVBgPD2UzQOlyb0QXYzJAAGkVjoxXNEAbKNwSRuAyQBPWFRCejTZAtB8jAEsyOECrke1b4WA2QBTRyJECVzVAmlxBruPVOEAs868+u5A2QE3OC9AufTdAluFT1JEhNkDLjZet9dc3QCv4V51EkzVAL20J2DV2N0CTx0EmuoIzQHNdvxOpsjlAIK2B9VA/NkAo7uQHE5A1QEhgsDueezVAnKA4HhDROUAf/Jc3jjYzQLA4g09zFDdAhEuRvEbVOUD60OoKiLE2QFLbYBGEoDZAS1hHsst8NEA4VZhGDPQyQI/1M1GPnTNAMhnnAL/9NEA=\",\"dtype\":\"float64\",\"shape\":[100]}}},\"id\":\"fe090d24-167a-423c-a7b7-d57ec7351e4f\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"e9253670-7df8-4d78-9647-b8ec106b6b8a\",\"type\":\"PanTool\"},{\"id\":\"7ea125e9-3437-413b-b51d-ed540d0e93f7\",\"type\":\"WheelZoomTool\"},{\"id\":\"4aeecad2-d7d5-4d05-8e0a-5a0fb1c7d483\",\"type\":\"BoxZoomTool\"},{\"id\":\"c5d27f91-be5a-4287-bcf1-48591507bdd4\",\"type\":\"SaveTool\"},{\"id\":\"4070654b-1511-4b66-9a68-95f2521617e9\",\"type\":\"ResetTool\"},{\"id\":\"dac4d1d2-6ca9-4889-9972-3455884ae92e\",\"type\":\"HelpTool\"}]},\"id\":\"84f77da6-1fe7-44fc-a393-155b3a5086f9\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"fe090d24-167a-423c-a7b7-d57ec7351e4f\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"06625c80-04ad-4db1-9397-9061d04c623e\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null},\"id\":\"acf3a4cc-f436-4fc0-bd51-bee76e239b2e\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"db755a54-7885-4b05-be3b-15b56f7f1feb\",\"type\":\"ToolEvents\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.7},\"fill_color\":{\"value\":\"#f22c40\"},\"line_color\":{\"value\":\"#f22c40\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x_values\"},\"y\":{\"field\":\"y_values\"}},\"id\":\"06625c80-04ad-4db1-9397-9061d04c623e\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"a384febe-8247-4662-b7ec-37a9c1134805\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"90d3e7c8-0a96-4563-a70e-7712d596a28f\",\"type\":\"ResetTool\"},{\"attributes\":{\"overlay\":{\"id\":\"8a7729d3-9c4c-49ea-942b-ab06f350d059\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"4aeecad2-d7d5-4d05-8e0a-5a0fb1c7d483\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1f8b59e1-a780-4022-88e2-9cf259c456b1\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"16e57199-11d7-4a90-95d9-995a82d53ea4\",\"type\":\"BasicTicker\"}},\"id\":\"b6918fce-8674-4eac-a460-070a4c501bdd\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"right\",\"top\",\"left\"],\"data\":{\"left\":{\"__ndarray__\":\"txP/e2j2OEDQoa5t9/c4QOkvXl+G+ThAAr4NURX7OEAbTL1CpPw4QDTabDQz/jhATmgcJsL/OEBn9ssXUQE5QICEewngAjlAmRIr+24EOUA=\",\"dtype\":\"float64\",\"shape\":[10]},\"right\":{\"__ndarray__\":\"0KGubff3OEDpL15fhvk4QAK+DVEV+zhAG0y9QqT8OEA02mw0M/44QE5oHCbC/zhAZ/bLF1EBOUCAhHsJ4AI5QJkSK/tuBDlAsqDa7P0FOUA=\",\"dtype\":\"float64\",\"shape\":[10]},\"top\":[1,2,4,8,17,20,26,10,7,5]}},\"id\":\"e5733881-dc18-4582-808c-754f7fd2998a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis_label\":\"m_cat [mag]\",\"formatter\":{\"id\":\"4b4195d8-8c40-492c-b7eb-2cceba724000\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"8eb632a1-815c-4d95-b436-85afd335d6e6\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"04b8552b-7bab-4d46-bd50-a515b5bc131c\",\"type\":\"BasicTicker\"}},\"id\":\"d4eaffe1-a785-47cf-9afd-929fa571be01\",\"type\":\"LinearAxis\"}],\"root_ids\":[\"58dcb1ab-b444-4dfc-83d6-b84acc86cb36\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.5\"}};\n",
       "            var render_items = [{\"docid\":\"b35a5cbe-3e75-48ce-b7fa-3a1fadd249d4\",\"elementid\":\"1f651d4d-52ce-4eba-813d-e1d37cf70529\",\"modelid\":\"58dcb1ab-b444-4dfc-83d6-b84acc86cb36\",\"notebook_comms_target\":\"3db116e8-b3b2-4ff3-a261-eb5792575023\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "          };\n",
       "          if (document.readyState != \"loading\") fn();\n",
       "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "        })();\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === true) {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (force !== true) {\n",
       "        var cell = $(document.getElementById(\"1f651d4d-52ce-4eba-813d-e1d37cf70529\")).parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><code>&lt;Bokeh Notebook handle for <strong>In[43]</strong>&gt;</code></p>"
      ],
      "text/plain": [
       "<bokeh.io._CommsHandle at 0x117ac9b38>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot side-by-side\n",
    "show(row(p, h), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The key to building useful plots is packing the right blob data with measurements to begin with. As you write your code, imagine what plots might usefully augment metric measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and outlook\n",
    "\n",
    "This technical note has demonstrated the full usage cycle of `lsst.verify`:\n",
    "\n",
    "* Defining metrics.\n",
    "* Defining specifications of metrics.\n",
    "* Measuring metrics.\n",
    "* Associating extra datasets with measurements.\n",
    "* Analyzing verification pipeline jobs, including building pass/fail reports and making plots.\n",
    "\n",
    "We encourage Data Management engineers and scientists to consider how you might instrument your own code, particularly pipeline Tasks, with verification measurements. By systematically monitoring performance metrics in your code, you will gain a clearer picture of how code developing is affecting your systems.\n",
    "\n",
    "This technical note has only shown local usage patterns with the `lsst.verify` framework. We are integrating `lsst.verify` with the [SQUASH](https://squash.lsst.codes) dashboard application. With SQUASH, you metric measurements are centrally available to the whole organization. We believe that `lsst.verify` and SQUASH will become an everday service for DM developers to ensure that code contributions do not introduce adverse performance side-effects across teh Stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
